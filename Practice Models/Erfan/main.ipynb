{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#data = pq.read_table('/workspaces/NLP-Polimi-Project/Practice Models/Felipe/dataset/train/data-00000-of-00001.arrow').to_pandas()\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#with open('/workspaces/NLP-Polimi-Project/Practice Models/Felipe/dataset/train/data-00000-of-00001.arrow', 'rb') as f:\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#    reader = ipc.open_file(f)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#    data = reader.read_all().to_pandas()\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marrow\u001b[39m\u001b[38;5;124m'\u001b[39m, data_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workspaces/NLP-Polimi-Project/Practice Models/Felipe/dataset/train/data-00000-of-00001.arrow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m data \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Split the data into train and test sets\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.ipc as ipc\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the Med-BERT model and tokenizer\n",
    "model_name = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Load the dataset from the saved arrow file\n",
    "dataset = Dataset.from_file('/workspaces/NLP-Polimi-Project/Practice Models/Erfan/Data/train/data-00000-of-00001.arrow')\n",
    "\n",
    "# Convert to Pandas DataFrame for analysis\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "print(f\"Dataset contains {len(df)} documents.\")\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the questions\n",
    "tokenized_train_questions = train_data['question'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "tokenized_test_questions = test_data['question'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "\n",
    "# Generate answers using Med-BERT for train and test sets\n",
    "generated_train_answers = []\n",
    "generated_test_answers = []\n",
    "\n",
    "for tokens in tokenized_train_questions:\n",
    "    input_ids = torch.tensor([tokens])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        generated_train_answers.append(embeddings.tolist())\n",
    "\n",
    "for tokens in tokenized_test_questions:\n",
    "    input_ids = torch.tensor([tokens])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        generated_test_answers.append(embeddings.tolist())\n",
    "\n",
    "# Calculate cosine similarity between generated answers and ground truth answers for train and test sets\n",
    "ground_truth_train_answers = train_data['answer']\n",
    "ground_truth_test_answers = test_data['answer']\n",
    "\n",
    "cosine_similarities_train = []\n",
    "cosine_similarities_test = []\n",
    "\n",
    "for generated_answer, ground_truth_answer in zip(generated_train_answers, ground_truth_train_answers):\n",
    "    cosine_similarities_train.append(cosine_similarity([generated_answer], [ground_truth_answer])[0][0])\n",
    "\n",
    "for generated_answer, ground_truth_answer in zip(generated_test_answers, ground_truth_test_answers):\n",
    "    cosine_similarities_test.append(cosine_similarity([generated_answer], [ground_truth_answer])[0][0])\n",
    "\n",
    "# Other performance metrics for train and test sets\n",
    "# Add your own metrics here\n",
    "\n",
    "# Plotting for train and test sets\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(cosine_similarities_train, bins=10)\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Cosine Similarity (Train)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(cosine_similarities_test, bins=10)\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Cosine Similarity (Test)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Add other plots here for train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Load the Med-BERT pretrained model\n",
    "model = SentenceTransformer('med-bert-base-uncased')\n",
    "\n",
    "# Encode the questions using the Med-BERT model\n",
    "question_embeddings = model.encode(df['question'].tolist())\n",
    "\n",
    "# Generate answers using the Med-BERT model\n",
    "generated_answers = []\n",
    "for question_embedding in question_embeddings:\n",
    "    # TODO: Use the Med-BERT model to generate an answer for each question\n",
    "    generated_answers.append(generated_answer)\n",
    "\n",
    "# Calculate cosine similarity between generated answers and actual answers\n",
    "cosine_similarities = cosine_similarity(question_embeddings, model.encode(df['answer'].tolist()))\n",
    "\n",
    "# Measure the performance of the model using other methods\n",
    "# TODO: Add other performance measurement methods here\n",
    "\n",
    "# Plotting\n",
    "# Plot the cosine similarities\n",
    "plt.hist(cosine_similarities.flatten(), bins=10)\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Cosine Similarities')\n",
    "plt.show()\n",
    "\n",
    "# TODO: Add other plots to showcase important aspects of the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     26\u001b[0m input_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the first line treatment for Ureaplasma urealyticum?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 27\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "Cell \u001b[0;32mIn[4], line 21\u001b[0m, in \u001b[0;36mgenerate_output\u001b[0;34m(input_string)\u001b[0m\n\u001b[1;32m     18\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Convert the embeddings back to text format\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m text_output \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text_output\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3836\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3833\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3834\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3836\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3840\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3841\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/tokenization_utils.py:1001\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode\u001b[39m(\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    993\u001b[0m     token_ids: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    998\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_use_source_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_source_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1001\u001b[0m     filtered_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1002\u001b[0m     legacy_added_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_encoder\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens) \u001b[38;5;241m|\u001b[39m {\n\u001b[1;32m   1003\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_special_tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(token) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m   1004\u001b[0m     }\n\u001b[1;32m   1005\u001b[0m     \u001b[38;5;66;03m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;66;03m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;66;03m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/tokenization_utils.py:976\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    974\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[0;32m--> 976\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_ids:\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'list'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate answer using BERT Q&A model\n",
    "def generate_answer(question, context):\n",
    "    # Tokenize the question and context\n",
    "    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors='pt')\n",
    "\n",
    "    # Get the input IDs and attention mask\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    # Generate answer using the BERT Q&A model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        start_scores = outputs.start_logits\n",
    "        end_scores = outputs.end_logits\n",
    "\n",
    "    # Find the start and end indices of the answer span\n",
    "    start_index = torch.argmax(start_scores)\n",
    "    end_index = torch.argmax(end_scores)\n",
    "\n",
    "    # Convert the token indices to actual tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze())\n",
    "    answer = tokenizer.convert_tokens_to_string(tokens[start_index:end_index+1])\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "context = \"BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained model for natural language processing.\"\n",
    "question = \"What is BERT?\"\n",
    "answer = generate_answer(question, context)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
